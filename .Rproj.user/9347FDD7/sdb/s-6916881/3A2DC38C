{
    "contents" : "# This assignment is pretty simple -- scrape the full text of 100 bills \n# inrtroduced in the Senate in the 112th congress and count the number of unique\n# words.  Once you have done this, I encourage you to go further, look for key\n# words and try to make some cool looking output. Style points are where it is \n# at!\n\n\n# Follow the template below but copy and paste all of the code into a script \n# file that you push to your Github repo after completing each step.\n\nrm(list = ls())\n# load the necessary libararies\nlibrary(stringr)\nlibrary(scrapeR)\n\n# Load in the bill urls -- you may need to set your working directory or alter \n# the path below\nload(\"./Data/Bill_URLs.Rdata\")\n\n# Try visiting the webiste, you will see that these URL's are from a beta \n# version. The URLs will look like:\n# http://beta.congress.gov/bill/112th-congress/senate-bill/886\n# What we actually want is something of the form:\n# https://www.congress.gov/bill/112th-congress/senate-bill/886/text?format=txt\n# we will need to loop through the text and replace the beginning \"http://beta.\"\n# with \"https://www.\" and then we will need to paste on \"/text?format=txt\" at \n# the end of each string. \n\n#replace part at beginning\nfixedURLs = vector(mode = \"character\", length = length(Bill_URLs))\nfor (i in 1:length(Bill_URLs)){\n  fixedURLs[i] = str_replace(Bill_URLs[i], \"http://beta.\", \"https://www.\")\n}\n\n#append new ending\nfor (i in 1:length(fixedURLs)){\n  fixedURLs[i] = paste(fixedURLs[i], \"/text?format=txt\", sep = \"\")\n}\n\n# Once you have the right URLs, you will want to scrape the web pages. Lets \n# start with a function adapted from the intermediate workshop:\nscrape_page <- function(url){\n  \n  # Print out the input name\n  cat(url, \"\\n\")\n  \n  # Make the input name all lowercase\n  url <- tolower(url)\n  \n  # Downloads the web page source code\n  page <- getURL(url, .opts = list(ssl.verifypeer = FALSE))\n  \n  # Split on newlines\n  page <- str_split(page,'\\n')[[1]]\n  \n  # Start of bill text \n  start <- grep(\"112th CONGRESS\",page)\n  \n  # End of bill text\n  end <- grep(\"&lt;all&gt;\",page)\n  \n  if(length(end) > 0 & length(start) > 0){\n    # Get just the text\n    print(start)\n    print(end)\n    if(!is.na(start) & !is.na(end)){\n      if(start < end & start > 0 & end > 0){\n        bill_text <- page[start:end]\n      }else{\n        bill_text <- \"\"\n      }\n    }else{\n      bill_text <- \"\"\n    }\n  }else{\n    bill_text <- \"\"\n  }\n  \n  # Save to a named list object\n  to_return <- list(page = page, text = bill_text)\n  \n  # return the list\n  return(to_return)\n}\n\n# test it out, take a look at the \n#test <- scrape_page( url = \"https://www.congress.gov/bill/112th-congress/senate-bill/886/text?format=txt\")\n\n# Now you will need to create a list object to store the data in, and loop over \n# URLS to store the data in the list. You will probably want to save your data\n# as an .Rdata object using save() at this point. One important point is that \n# you NEED TO INCLUDE a Sys.sleep(5) in your scraping loop so you do not go too\n# fast and overwhelm the congress.gov servers. Going too fast can land you in \n# BIG legal trouble (that is called a \"denial of service attack\") so jsut keep \n# things at a reasonable pace. \n#'\n\n\n#create an empty list\nrawData = list()\n#this loop fills the rawData list with a list of two vectors for each URL (in this case, 100 lists); the first vector is a vector of strings representing the whole page, separated by lines, the second vector is a vector of strings representing the bill alone and is named $text\nfor (i in 1:length(fixedURLs)){\n  rawData[[i]] = scrape_page(url = fixedURLs[i])\n  Sys.sleep(2)\n}\n\nnames(rawData) = fixedURLs\n\n# Now you will need to deal with the text... This function is being given to you\n# as a way to clean up a single string.\n#TEST: string <- \"inspections..#$^relocation..???!!!}{[]()\"\n#this returns a vector of characters, separated by whitespace; in other words, it turns a single character string into a vector of \"words\"\nClean_String <- function(string){\n  # Lowercase\n  temp <- tolower(string)\n  # Remove everything that is not a number letter ? or !\n  temp <- stringr::str_replace_all(temp,\"[^a-zA-Z\\\\s:\\\\?\\\\!]\", \" \")\n  # Shrink down to just one white space\n  temp <- stringr::str_replace_all(temp,\"[\\\\s]+\", \" \")\n  # Split it\n  temp <- stringr::str_split(temp, \" \")[[1]]\n  # Get rid of trailing \"\" if necessary\n  indexes <- which(temp == \"\")\n  if(length(indexes) > 0){\n    temp <- temp[-indexes]\n  }\n  return(temp)\n}\n\n# The function above will clean one string but you have lots. You can deal with \n# them by filling in the function below:\n#'So I guess this will take a vector of strings from a single bill and return ... what? let's look. \nClean_Text_Block <- function(text){\n  if(length(text) <= 1){\n    \n    # Check to see if there is any text at all with another conditional\n    #Sam's translation: check to see if the vector contains anything at all.\n    if(length(text) == 0){\n      numtokens = 0\n      numuniquetokens = 0\n      tokens = NA\n    }\n    \n    # If there is , and only only one line of text then tokenize it \n    #Sam's code: this creates a vector of words and assigns it to the variable tokens\n    #unfortunately I won't actually know if this works because AFAIK there are no one-line bills here!\n    tokens = Clean_String(text[1])\n    \n  }else{\n    \n    # Get rid of blank lines\n    indexes <- which(text == \"\")\n    if(length(indexes) > 0){\n      text <- text[-indexes]\n    }\n    \n    # Loop through the lines in the text and use the append() function to \n    # add them to a vector \n    #Sam's code: create an empty vector\n    tokens = c()\n    #Sam's code: for each element in the vector of strings, tokenize it and add it to the end of our running vector called \"tokens\"\n    #As far as I can tell, this would work if there were just one element in the vector, so I'm not sure why I needed to write the extra code above.\n    for(i in 1:length(text)){\n      tokens = append(tokens, Clean_String(text[i]))\n    }\n    \n  }\n  \n  # Calculate the number of tokens and unique tokens and return them in a \n  # named list object with the tokens using something like \n  # to_return <- list(count = my_count, ...) and then return(to_return)\n  #Sam's code:Okay this calculates the number of tokens\n  numtokens = length(tokens)\n  numuniquetokens = length(unique(tokens))\n  #Now I just need to calculate the number of unique tokens ... I can probably use unique and one of the grep or grepl functions we learned, but I bet there's a more efficient way to do it. \n  toReturn = list(total = numtokens, unique = numuniquetokens, words = tokens)\n  return(toReturn)\n}\n  \n\n# Now that we have a function to do this, we will need to loop over the 100 \n# bills and save the results into a new list object. \nwordCountList = list()\nfor(i in 1:length(rawData)){\n  wordCountList[[i]] = Clean_Text_Block(rawData[[i]]$text)\n}\n\n\n# Once we have done that, it is time to add up the count variables and maybe \n# plot them \n\ntotalWords = 0\nfor (i in 1:length(wordCountList)){\n  totalWords = totalWords + wordCountList[[i]]$total\n}\n\ntotalUniqueWords = 0\nfor (i in 1:length(wordCountList)){\n  totalUniqueWords = totalUniqueWords + wordCountList[[i]]$unique\n}\n\ntotalvector = c()\nfor (i in 1:length(wordCountList)){\n  totalvector[i] = wordCountList[[i]]$total\n}\n\nuniquevector = c()\nfor (i in 1:length(wordCountList)){\n  uniquevector[i] = wordCountList[[i]]$unique\n}\n\nplot(totalvector, uniquevector, type = \"p\", main = \"Association between total number of words in a bill and unique number of words\", xlab = \"Total\", ylab = \"Unique\")\n\n####### Stretch goals #######\n# So, you finished all of that, what next? Here are some other useful things you\n# should try to do.\n\n# 1. Write a function that counts the number of times a given word appears in \n# all 100 bills.\n\nwordAppears = function(find, wordList){\n  logicalvec = grepl(pattern = find, wordList)\n  return(sum(logicalvec))\n}\n\nwordInAllBills = function(word){\n  count = 0\n  for(i in 1:length(wordCountList)){\n    count = count + wordAppears(find = word, wordList = wordCountList[[i]]$words)\n  }\n  return(count)\n}\n\n#save(rawData, wordCountList, totalUniqueWords, totalWords, totalvector, uniquevector, file = \"homework 1 output\")\n# 2. Take a look at the raw html for a bill and try to write a function that \n# that will extract some other pieces of metadata from it (such as the date\n# it was introduced, the author, and whether it made it to the floor) and then\n# save all of that data into another dataframe.\n# 3. Generate a dataframe with two columns, the first has a word and the second\n# has the number of times it appears.\n# 4. Create an dotplot using ggplot2 showing differences in the use of some \n# word(s) across different bills by some descriptive feature (perhaps author \n# party?)\n\n\n\n\n",
    "created" : 1433185713674.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "112|33|127|0|\n132|35|177|0|\n",
    "hash" : "3086238146",
    "id" : "3A2DC38C",
    "lastKnownWriteTime" : 1433253609,
    "path" : "~/GitHub/ISSR_DataSci_Workshop/Scripts/Day_One_Exercise.R",
    "project_path" : "Scripts/Day_One_Exercise.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}